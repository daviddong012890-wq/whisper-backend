import express from "express";
import cors from "cors";
import multer from "multer";
import fs from "fs";
import path from "path";
import ffmpeg from "fluent-ffmpeg";
import ffmpegStatic from "ffmpeg-static";
import axios from "axios";
import FormData from "form-data";
import nodemailer from "nodemailer";
import crypto from "crypto";
import http from "http";
import https from "https";
import { Pool } from "pg";
import { Document, Packer, Paragraph } from "docx";

// ---------- notify PHP (worker-consume.php) ----------
const CONSUME_URL = process.env.CONSUME_URL || "";
const WORKER_SHARED_KEY = process.env.WORKER_SHARED_KEY || "";
const CALLBACK_URL = process.env.CALLBACK_URL || ""; // <-- ADDED

async function consume(payload) {
  if (!CONSUME_URL) return;
  try {
    await axios.post(CONSUME_URL, payload, {
      headers: WORKER_SHARED_KEY ? { "X-Worker-Key": WORKER_SHARED_KEY } : {},
      timeout: 10000,
    });
    console.log("‚Üí consume() POST ok");
  } catch (e) {
    console.error("consume() error:", e?.response?.status || "", e?.message || e);
  }
}

// ---------- NEW: notify PHP dashboard (worker-callback.php) ----------
async function updateStatus(jobId, status, durationSec = 0) {
  if (!CALLBACK_URL) return;
  try {
    await axios.post(
      CALLBACK_URL,
      new URLSearchParams({
        job_id: jobId,
        status: status,
        duration_sec: durationSec.toString(),
      }),
      {
        headers: {
          "X-Worker-Key": WORKER_SHARED_KEY,
          "Content-Type": "application/x-www-form-urlencoded",
        },
        timeout: 10000,
      }
    );
    console.log(`‚Üí updateStatus(${jobId}, ${status}) ok`);
  } catch (err) {
    console.error("updateStatus error:", err?.response?.status, err?.message || err);
  }
}

// ---------- app / setup ----------
const app = express();
const allowedOrigins = ["https://voixl.com", "https://www.voixl.com"];
app.use(
  cors({
    origin: function (origin, callback) {
      if (!origin || allowedOrigins.indexOf(origin) !== -1) {
        callback(null, true);
      } else {
        callback(new Error("Not allowed by CORS"));
      }
    },
  })
);
app.options("*", cors());
app.use(express.json({ limit: "1mb" }));

// ===== Upload-only mode =====
const MAX_UPLOAD_BYTES = Number(
  process.env.MAX_UPLOAD_BYTES || 1.5 * 1024 * 1024 * 1024
); // 1.5 GB default
const upload = multer({
  dest: "/tmp",
  limits: { fileSize: MAX_UPLOAD_BYTES },
  fileFilter: (_req, file, cb) => {
    const ok =
      (file.mimetype || "").startsWith("audio/") ||
      (file.mimetype || "").startsWith("video/");
    if (!ok) return cb(new Error("Only audio/video files are allowed."));
    cb(null, true);
  },
});
ffmpeg.setFfmpegPath(ffmpegStatic);

// ---------- env checks ----------
const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
const GMAIL_USER = process.env.GMAIL_USER;
const GMAIL_PASS = process.env.GMAIL_PASS;
const LOCAL_TZ = process.env.LOCAL_TZ || "America/Los_Angeles";
const FROM_EMAIL = process.env.FROM_EMAIL || GMAIL_USER;
const FROM_NAME = process.env.FROM_NAME || "ÈÄêÂ≠óÁ®øÁî¢ÁîüÂô®";

function fatal(m) {
  console.error("‚ùå " + m);
  process.exit(1);
}
if (!OPENAI_API_KEY) fatal("Missing OPENAI_API_KEY");
if (!GMAIL_USER || !GMAIL_PASS) fatal("Missing GMAIL_USER or GMAIL_PASS");

// ---------- Postgres pool ----------
/**
 * Prefer Render‚Äôs DATABASE_URL. Fallback to DB_* if provided.
 * DATABASE_URL example: postgres://user:pass@host:5432/dbname
 */
const DATABASE_URL = process.env.DATABASE_URL || "";
const DB_HOST = process.env.DB_HOST || "";
const DB_PORT = Number(process.env.DB_PORT || 5432);
const DB_USER = process.env.DB_USER || "";
const DB_PASS = process.env.DB_PASS || "";
const DB_NAME = process.env.DB_NAME || "";
const DB_SSL = (process.env.DB_SSL || "true").toLowerCase() === "true";

const pool =
  DATABASE_URL
    ? new Pool({
        connectionString: DATABASE_URL,
        ssl: DB_SSL ? { rejectUnauthorized: false } : undefined,
        max: 10,
      })
    : new Pool({
        host: DB_HOST,
        port: DB_PORT,
        user: DB_USER,
        password: DB_PASS,
        database: DB_NAME,
        ssl: DB_SSL ? { rejectUnauthorized: false } : undefined,
        max: 10,
      });

pool
  .query("SELECT 1")
  .then(() => console.log("‚úÖ DB connectivity OK (Postgres)"))
  .catch((e) => {
    console.error("‚ùå DB connectivity failed:", e.code || "", e.message);
  });

/** -------------------------------------------------------
 *  Ensure required tables exist (auto-migrate on boot)
 *  - jobs           (requestid, status, steps jsonb, error, created_at)
 *  - transcriptions (columns your code writes to)
 * ------------------------------------------------------ */
async function ensureSchema() {
  // jobs table: unquoted names -> lowercase (requestid)
  await pool.query(`
    CREATE TABLE IF NOT EXISTS jobs (
      requestid   TEXT PRIMARY KEY,
      status      TEXT NOT NULL,
      steps       JSONB NOT NULL DEFAULT '[]'::jsonb,
      error       TEXT,
      created_at  TIMESTAMPTZ NOT NULL DEFAULT now()
    );
    CREATE INDEX IF NOT EXISTS idx_jobs_created_at ON jobs(created_at);
  `);

  // transcriptions table with the columns your inserts expect
  await pool.query(`
    CREATE TABLE IF NOT EXISTS transcriptions (
      id                 BIGSERIAL PRIMARY KEY,
      timestamputc       TIMESTAMPTZ NOT NULL,
      timestamplocal     TEXT NOT NULL,
      email              TEXT NOT NULL,
      jobseconds         INTEGER NOT NULL,
      cumulativeseconds  INTEGER NOT NULL,
      minutes            INTEGER NOT NULL,
      cumulativeminutes  INTEGER NOT NULL,
      filename           TEXT NOT NULL,
      filesizemb         NUMERIC(10,2) NOT NULL,
      language           TEXT NOT NULL,
      requestid          TEXT NOT NULL,
      processingms       INTEGER NOT NULL,
      succeeded          BOOLEAN NOT NULL,
      errormessage       TEXT NOT NULL,
      model              TEXT NOT NULL,
      filetype           TEXT NOT NULL
    );
    CREATE INDEX IF NOT EXISTS idx_trans_email ON transcriptions(email);
    CREATE INDEX IF NOT EXISTS idx_trans_reqid ON transcriptions(requestid);
    CREATE INDEX IF NOT EXISTS idx_trans_succeeded ON transcriptions(succeeded);
  `);

  console.log("‚úÖ Schema ready (jobs, transcriptions)");
}

// init schema at boot (fail fast if cannot create)
await ensureSchema().catch((e) => {
  console.error("‚ùå Schema init failed:", e);
  process.exit(1);
});

// ---------- mailer ----------
const mailer = nodemailer.createTransport({
  host: "smtp.gmail.com",
  port: 465,
  secure: true,
  auth: { user: GMAIL_USER, pass: GMAIL_PASS },
});

// ---------- small utils ----------
const OPENAI_AUDIO_MAX = 25 * 1024 * 1024; // OpenAI per-file limit (25 MB)
const TARGET_MAX_BYTES = 24 * 1024 * 1024; // aim just under
const MIN_SEG_SECONDS = 420; // 7 min
const MAX_SEG_SECONDS = 900; // 15 min
const DEFAULT_SEG_SECONDS = 900;

function statBytes(p) {
  try {
    return fs.statSync(p).size;
  } catch {
    return 0;
  }
}
function fmtLocalStamp(d) {
  const parts = new Intl.DateTimeFormat("en-US", {
    timeZone: LOCAL_TZ,
    year: "numeric",
    month: "short",
    day: "numeric",
    hour: "2-digit",
    minute: "2-digit",
    second: "2-digit",
    hour12: true,
  }).formatToParts(d);
  let Y, M, D, hh, mm, ss, ap;
  for (const p of parts) {
    if (p.type === "year") Y = p.value;
    else if (p.type === "month") M = p.value;
    else if (p.type === "day") D = p.value;
    else if (p.type === "hour") hh = p.value;
    else if (p.type === "minute") mm = p.value;
    else if (p.type === "second") ss = p.value;
    else if (p.type === "dayPeriod") ap = p.value.toUpperCase();
  }
  return `${Y} ${M} ${D} ${hh}:${mm}:${ss} ${ap}`;
}
function secsToSheetMinutes(sec) {
  return Math.max(1, Math.ceil((sec || 0) / 60));
}
function sleep(ms) {
  return new Promise((r) => setTimeout(r, ms));
}

// ---------- keep-alive axios for OpenAI (reduces "socket hang up") ----------
const httpAgent = new http.Agent({ keepAlive: true, maxSockets: 50 });
const httpsAgent = new https.Agent({ keepAlive: true, maxSockets: 50 });

const axiosOpenAI = axios.create({
  httpAgent,
  httpsAgent,
  timeout: 120000, // 120s hard timeout
  maxContentLength: Infinity,
  maxBodyLength: Infinity,
  headers: {
    Connection: "keep-alive",
    Accept: "application/json",
  },
});

// ---------- DB helpers (Postgres) ----------
async function createJob(id) {
  const step = { at: new Date().toISOString(), text: "Job accepted by server." };
  await pool.query(
    `INSERT INTO jobs (requestid, status, steps, created_at)
     VALUES ($1, $2, $3::jsonb, now())
     ON CONFLICT (requestid)
     DO UPDATE SET status = EXCLUDED.status, steps = EXCLUDED.steps`,
    [id, "accepted", JSON.stringify([step])]
  );
  console.log(`[${id}] Job created in database.`);
}

async function addStep(id, text) {
  const step = { at: new Date().toISOString(), text };
  await pool.query(
    `UPDATE jobs
       SET steps = COALESCE(steps, '[]'::jsonb) || $1::jsonb
     WHERE requestid = $2`,
    [JSON.stringify([step]), id]
  );
  console.log(`[${id}] ${text}`);
}

async function setJobStatus(id, status, error = null) {
  await pool.query(
    `UPDATE jobs SET status = $1, error = $2 WHERE requestid = $3`,
    [status, error, id]
  );
}

// status endpoint
app.get("/status", async (req, res) => {
  const id = (req.query.id || "").toString();
  if (!id) return res.status(400).json({ error: "Missing id" });
  const { rows } = await pool.query(
    `SELECT requestid, status, steps, error, created_at
       FROM jobs
      WHERE requestid = $1
      LIMIT 1`,
    [id]
  );
  const j = rows[0];
  if (!j) return res.status(404).json({ error: "Not found" });
  const steps =
    Array.isArray(j.steps)
      ? j.steps
      : (() => {
          try {
            return JSON.parse(j.steps || "[]");
          } catch {
            return [];
          }
        })();
  res.json({ ...j, steps });
});

// ---------- media analysis ----------
function ffprobeDurationSeconds(filePath) {
  return new Promise((resolve, reject) => {
    ffmpeg.ffprobe(filePath, (err, meta) => {
      if (err) return reject(err);
      resolve(Number(meta?.format?.duration) || 0);
    });
  });
}

// ---------- bitrate planning ----------
function estimateSizeBytes(seconds, kbps) {
  return Math.ceil(seconds * (kbps * 1000) / 8);
}
function chooseBitrateAndSplit(seconds, candidateKbps = [96, 64, 48, 32, 24, 16]) {
  for (const kb of candidateKbps) {
    const est = estimateSizeBytes(seconds, kb);
    if (est <= TARGET_MAX_BYTES) {
      return { kbps: kb, needsSplit: false, estBytes: est };
    }
  }
  return {
    kbps: candidateKbps[candidateKbps.length - 1],
    needsSplit: true,
    estBytes: estimateSizeBytes(seconds, candidateKbps[candidateKbps.length - 1]),
  };
}
function computeSegmentSeconds(kbps) {
  const seconds = Math.floor(TARGET_MAX_BYTES / ((kbps * 1000) / 8));
  return Math.max(MIN_SEG_SECONDS, Math.min(MAX_SEG_SECONDS, seconds || DEFAULT_SEG_SECONDS));
}

// ---------- single-pass encode helpers ----------
async function encodeSingleMp3(inPath, outMp3, kbps, requestId) {
  addStep(requestId, `Encode MP3 @ ${kbps} kbps (single file)‚Ä¶`);
  await new Promise((resolve, reject) => {
    ffmpeg(inPath)
      .noVideo()
      .audioFilters(["highpass=f=200", "lowpass=f=3800", "dynaudnorm"])
      .outputOptions([
        "-ac",
        "1",
        "-ar",
        "16000",
        "-b:a",
        `${kbps}k`,
        "-codec:a",
        "libmp3lame",
      ])
      .save(outMp3)
      .on("end", resolve)
      .on("error", reject);
  });
  return outMp3;
}
async function encodeAndSegmentMp3(inPath, outPattern, kbps, segmentSeconds, requestId) {
  addStep(requestId, `Encode+Segment MP3 @ ${kbps} kbps, ~${segmentSeconds}s/part‚Ä¶`);
  await new Promise((resolve, reject) => {
    ffmpeg(inPath)
      .noVideo()
      .audioFilters(["highpass=f=200", "lowpass=f=3800", "dynaudnorm"])
      .outputOptions([
        "-ac",
        "1",
        "-ar",
        "16000",
        "-b:a",
        `${kbps}k`,
        "-codec:a",
        "libmp3lame",
        "-f",
        "segment",
        "-segment_time",
        String(segmentSeconds),
        "-reset_timestamps",
        "1",
      ])
      .save(outPattern)
      .on("end", resolve)
      .on("error", reject);
  });
  const dir = path.dirname(outPattern);
  const base = path.basename(outPattern).split("%")[0]; // prefix before %03d
  const files = fs
    .readdirSync(dir)
    .filter((n) => n.startsWith(base) && n.endsWith(".mp3"))
    .map((n) => path.join(dir, n))
    .sort();
  return files;
}

// ---------- OpenAI (Whisper) ----------
async function openaiTranscribeVerbose(audioPath, requestId) {
  try {
    const fd = new FormData();
    fd.append("file", fs.createReadStream(audioPath), {
      filename: path.basename(audioPath),
    });
    fd.append("model", "whisper-1");
    fd.append("response_format", "verbose_json");
    fd.append("temperature", "0");
    const r = await axiosOpenAI.post(
      "https://api.openai.com/v1/audio/transcriptions",
      fd,
      {
        headers: {
          Authorization: `Bearer ${OPENAI_API_KEY}`,
          ...fd.getHeaders(),
        },
      }
    );
    return r.data;
  } catch (err) {
    console.error(
      `[${requestId}] Whisper transcribe error:`,
      err?.response?.status,
      err?.message
    );
    throw err;
  }
}

// ---------- retries & bounded concurrency ----------
function sleepMs(ms) {
  return new Promise((r) => setTimeout(r, ms));
}
async function withRetries(fn, { maxAttempts = 5, baseDelayMs = 700 } = {}) {
  let attempt = 0;
  while (true) {
    try {
      return await fn();
    } catch (e) {
      attempt++;
      const s = e?.response?.status;
      const retriable =
        s === 429 || (s >= 500 && s < 600) || e.code === "ECONNRESET" || e.code === "ETIMEDOUT";
      if (!retriable || attempt >= maxAttempts) throw e;
      const delay = Math.floor(baseDelayMs * Math.pow(2, attempt - 1) + Math.random() * 250);
      await sleepMs(delay);
    }
  }
}
async function runBounded(tasks, limit = 3) {
  const results = new Array(tasks.length);
  let next = 0,
    active = 0;
  return new Promise((resolve, reject) => {
    const launch = () => {
      if (next >= tasks.length && active === 0) return resolve(results);
      while (active < limit && next < tasks.length) {
        const idx = next++;
        active++;
        Promise.resolve()
          .then(() => tasks[idx]())
          .then((r) => {
            results[idx] = r;
          })
          .catch(reject)
          .finally(() => {
            active--;
            launch();
          });
      }
    };
    launch();
  });
}

// ---------- GPT translation (multilingual ‚Üí zh-TW, robust) ----------
async function gptTranslateFaithful(originalAll, requestId) {
  const systemPrompt = `‰Ω†ÊòØÂúãÈöõÊúÉË≠∞ÁöÑ‰∏ÄÁ∑öÂè£Á≠ÜË≠ØÂì°„ÄÇË´ãÊää‰ΩøÁî®ËÄÖÊèê‰æõÁöÑ„ÄåÂéüÊñá„ÄçÂÆåÊï¥ÁøªË≠ØÊàê„ÄåÁπÅÈ´î‰∏≠ÊñáÔºàÂè∞ÁÅ£ÊÖ£Áî®Ôºâ„Äç‰∏¶Âö¥Ê†ºÈÅµÂÆàÔºö

1) Âø†ÂØ¶ËΩâË≠ØÔºö‰∏çÂæóÂ¢ûÂà™„ÄÅ‰∏çÂæóËáÜÊ∏¨Ôºå‰∏çÂä†ÂÖ•‰ªª‰ΩïË©ïË´ñÔºõÂÉÖÂÅöÂøÖË¶ÅË™ûÂ∫èËàáË™ûÊ≥ïË™øÊï¥Ôºå‰Ωø‰∏≠ÊñáÂèØËÆÄ‰ΩÜ‰∏çÊÑèË≠Ø„ÄÇ
2) Âè•Â∫èËàáÊÆµËêΩÔºö‰æùÂéüÊñáÁöÑÈ†ÜÂ∫èËàáÂàÜÊÆµËº∏Âá∫Ôºõ‰øùÁïôÈáçË§á„ÄÅÂè£È†≠Ë™ûËàáË™ûÊ∞£Ë©ûÔºàÂ¶Ç„ÄåÂóØ„Äç„ÄåÂëÉ„ÄçÔºâÔºåÈô§ÈùûÂΩ±ÈüøÁêÜËß£ÊâçÂèØËºïÂæÆÂπ≥È†ÜÂåñ„ÄÇ
3) Â§öË™ûÂàáÊèõÔºö‰∏çË´ñÂéüÊñáÂá∫ÁèæÂì™‰∫õË™ûË®ÄÔºàÂ¶ÇËã±Êñá„ÄÅË•øÊñá„ÄÅÊ≥ïÊñá„ÄÅÂæ∑Êñá„ÄÅ‰∏≠ÊñáÁ≠âÔºâÔºå‰∏ÄÂæãË≠ØÁÇ∫ÁπÅÈ´î‰∏≠Êñá„ÄÇ
   - Â∞àÊúâÂêçË©ûËàáÂ∏∏Ë¶ãË≠ØÂêçÔºö‰ΩøÁî®Âè∞ÁÅ£ÊÖ£Áî®ÊàñÈÄöË°åÁöÑ‰∏≠ÊñáË≠ØÂêç„ÄÇ
   - Ëã•ÁÑ°Âõ∫ÂÆöË≠ØÂêçÔºöÊé°Èü≥Ë≠ØÊàñÊÑèË≠ØÔºå‰∏¶Âú®„ÄåÈ¶ñÊ¨°Âá∫Áèæ„ÄçÊñº‰∏≠ÊñáÂæåÂä†‰∏äÂéüÊñáÊã¨ËôüÔºå‰æãÂ¶ÇÔºöÊ°ëÂæ∑ÊãâÔºàSandraÔºâ„ÄÅÂì•ÂÄ´ÊØî‰∫ûÂ§ßÂ≠∏ÔºàColumbia UniversityÔºâ„ÄÇ
4) Êï∏Â≠óËàáÂñÆ‰ΩçÔºöÊï∏Â≠ó‰ΩøÁî®ÈòøÊãâ‰ºØÊï∏Â≠óÔºõÂ∫¶ÈáèË°°„ÄÅË≤®Âπ£Á≠âÊé°Âè∞ÁÅ£Â∏∏Áî®ÂØ´Ê≥ïÔºàÂÖ¨Èáå„ÄÅÂÖ¨Êñ§„ÄÅÁæéÂÖÉ‚Ä¶Ôºâ„ÄÇ
5) Ê®ôÈªûÔºö‰ΩøÁî®‰∏≠ÊñáÂÖ®ÂΩ¢Ê®ôÈªû„ÄÇ
6) ‰øùÁïô‰∏çÊáâÁøªÁöÑÂÖßÂÆπÔºöÁ∂≤ÂùÄ„ÄÅÈõªÂ≠êÈÉµ‰ª∂„ÄÅÊ™îÂêç„ÄÅÁ®ãÂºèÁ¢ºÁâáÊÆµ„ÄÅÊåá‰ª§„ÄÅÊ®°ÂûãÂêçÁ®±Á≠â‰ª•ÂéüÊ®£‰øùÁïôÔºàÂèØÈÖçÂêà‰∏≠ÊñáÊ®ôÈªûÔºâ„ÄÇ
7) Âè™Ëº∏Âá∫Ë≠ØÊñáÊ≠£ÊñáÔºö‰∏çË¶Å‰ªª‰ΩïË™™Êòé„ÄÅÊ®ôÈ°åÊàñË®ªËß£Ôºõ‰∏çË¶ÅÊëòË¶ÅÊàñÈáçÂØ´„ÄÇ
8) Ëã•ÂéüÊñáÊú¨Ë∫´ÊòØ‰∏≠ÊñáÔºöÁµ±‰∏ÄÁÇ∫Âè∞ÁÅ£ÊÖ£Áî®Ë©ûËàáÂÖ®ÂΩ¢Ê®ôÈªûÔºåÈÅøÂÖçÈÅéÂ∫¶ÊîπÂØ´„ÄÇ

Ë´ãÁõ¥Êé•Ëº∏Âá∫ÊúÄÁµÇË≠ØÊñá„ÄÇ`;

  const payload = {
    model: "gpt-4o-mini",
    temperature: 0,
    messages: [
      { role: "system", content: systemPrompt },
      { role: "user", content: originalAll || "" },
    ],
  };

  const resp = await withRetries(
    () =>
      axiosOpenAI.post("https://api.openai.com/v1/chat/completions", payload, {
        headers: { Authorization: `Bearer ${OPENAI_API_KEY}` },
        validateStatus: (s) => s >= 200 && s < 500,
      }),
    { maxAttempts: 5, baseDelayMs: 800 }
  ).catch((err) => {
    const s = err?.response?.status;
    const d = err?.response?.data;
    addStep(
      requestId,
      `GPT error ${s || "no-status"} ${
        typeof d === "string" ? d.slice(0, 180) : JSON.stringify(d || {}).slice(0, 180)
      }`
    );
    throw err;
  });

  return resp?.data?.choices?.[0]?.message?.content?.trim() || "";
}

// ---------- main processor ----------
async function processJob({ email, inputPath, fileMeta, requestId, jobId, token }) {
  await setJobStatus(requestId, "processing");
  await updateStatus(requestId, "processing"); // <-- ADDED

  addStep(
    requestId,
    `Processing: ${fileMeta.originalname} (${(fileMeta.size / 1024 / 1024).toFixed(2)} MB)`
  );

  const tempFiles = new Set([inputPath]);
  const started = Date.now();
  const model = "whisper-1";
  let language = "";
  const fileType = fileMeta.mimetype || "";
  const fileName = fileMeta.originalname || "upload";
  const fileSizeMB = Math.max(0.01, Math.round(((fileMeta.size || 0) / (1024 * 1024)) * 100) / 100);

  try {
    const durationSec = await ffprobeDurationSeconds(inputPath);
    addStep(requestId, `Detected duration: ${Math.round(durationSec)}s`);

    const { kbps, needsSplit } = chooseBitrateAndSplit(durationSec);
    addStep(
      requestId,
      `Chosen bitrate: ${kbps} kbps; ${needsSplit ? "will segment" : "single file"}.`
    );

    let parts = [];
    const tmpBase = `/tmp/${requestId}`;
    if (!needsSplit) {
      const singleOut = `${tmpBase}.${kbps}k.mp3`;
      tempFiles.add(singleOut);
      await encodeSingleMp3(inputPath, singleOut, kbps, requestId);
      const sz = statBytes(singleOut);
      addStep(requestId, `Encoded size: ${(sz / 1024 / 1024).toFixed(2)} MB`);
      if (sz > OPENAI_AUDIO_MAX) {
        addStep(requestId, "Single file still >25MB ‚Äî encoding again with segmentation ‚Ä¶");
        try {
          fs.unlinkSync(singleOut);
        } catch {}
        tempFiles.delete(singleOut);
        const segSec = computeSegmentSeconds(kbps);
        const pattern = `${tmpBase}.part-%03d.mp3`;
        const segs = await encodeAndSegmentMp3(inputPath, pattern, kbps, segSec, requestId);
        segs.forEach((p) => tempFiles.add(p));
        parts = segs;
      } else {
        parts = [singleOut];
      }
    } else {
      const segSec = computeSegmentSeconds(kbps);
      const pattern = `${tmpBase}.part-%03d.mp3`;
      const segs = await encodeAndSegmentMp3(inputPath, pattern, kbps, segSec, requestId);
      segs.forEach((p) => tempFiles.add(p));
      parts = segs;
    }

    // compute duration from encoded parts
    async function getSeconds(filePath) {
      return new Promise((resolve, reject) => {
        ffmpeg.ffprobe(filePath, (err, meta) => {
          if (err) return reject(err);
          resolve(Number(meta?.format?.duration) || 0);
        });
      });
    }
    let jobSeconds = 0;
    for (const p of parts) jobSeconds += await getSeconds(p);
    jobSeconds = Math.round(jobSeconds);

    const minutesForDb = secsToSheetMinutes(jobSeconds);

    // cumulative seconds (sum only succeeded jobs)
    let pastSeconds = 0;
    try {
      const { rows } = await pool.query(
        `SELECT COALESCE(SUM(jobseconds), 0)::int AS total
           FROM transcriptions
          WHERE email = $1 AND succeeded = true`,
        [email]
      );
      pastSeconds = Number(rows?.[0]?.total || 0);
    } catch (e) {
      console.error("‚ö†Ô∏è getPastSeconds query error:", e.message || e);
    }
    const cumulativeSeconds = pastSeconds + jobSeconds;
    const cumulativeMinutesForDb = secsToSheetMinutes(cumulativeSeconds);
    addStep(requestId, `Duration this job: ${jobSeconds}s; cumulative: ${cumulativeSeconds}s.`);

    // parallel transcription
    addStep(requestId, `Transcribing ${parts.length} part(s) in parallel (bounded)‚Ä¶`);
    const concurrency = Number(process.env.WHISPER_CONCURRENCY || 3);
    const tasks = parts.map((filePath, idx) => async () => {
      addStep(requestId, `Part ${idx + 1}/${parts.length} ‚Üí start`);
      const res = await withRetries(
        () => openaiTranscribeVerbose(filePath, requestId),
        {
          maxAttempts: 5,
          baseDelayMs: 700,
        }
      );
      addStep(requestId, `Part ${idx + 1}/${parts.length} ‚Üí done`);
      return res;
    });
    const results = await runBounded(tasks, concurrency);

    let originalAll = "";
    for (const verbose of results) {
      if (!language && verbose?.language) language = verbose.language;
      originalAll += (originalAll ? "\n\n" : "") + (verbose?.text || "");
    }

    // zh-TW faithful translation (multilingual) ‚Äî robust against socket hangups
    addStep(requestId, "Calling GPT ÂéüÊñá‚ÜíÁπÅ‰∏≠ (faithful, multilingual) ‚Ä¶");
    let zhTraditional = "";
    try {
      // If you fear extremely long prompts, you can cap length:
      // const inputForGpt = (originalAll || "").slice(0, 20000);
      const inputForGpt = originalAll || "";
      zhTraditional = await gptTranslateFaithful(inputForGpt, requestId);
      addStep(requestId, "ÁπÅ‰∏≠ done.");
    } catch (_) {
      addStep(requestId, "‚ö†Ô∏è GPT translation failed ‚Äî sending original only.");
      zhTraditional = "";
    }

    // email with attachments
    const localStamp = fmtLocalStamp(new Date());
    const attachmentText = `ÔºùÔºù ‰∏≠ÊñáÔºàÁπÅÈ´îÔºâ ÔºùÔºù
${zhTraditional}

ÔºùÔºù ÂéüÊñá ÔºùÔºù
${originalAll}
`;
    const safeBase =
      (fileName || "transcript").replace(/[^\w.-]+/g, "_").slice(0, 50) || "transcript";
    const txtName = `${safeBase}-${requestId}.txt`;
    const docxName = `${safeBase}-${requestId}.docx`;
    const doc = new Document({
      sections: [
        {
          children: [
            new Paragraph("ÔºùÔºù ‰∏≠ÊñáÔºàÁπÅÈ´îÔºâ ÔºùÔºù"),
            ...String(zhTraditional || "")
              .split("\n")
              .map((line) => new Paragraph(line)),
            new Paragraph(""),
            new Paragraph("ÔºùÔºù ÂéüÊñá ÔºùÔºù"),
            ...String(originalAll || "")
              .split("\n")
              .map((line) => new Paragraph(line)),
          ],
        },
      ],
    });
    const docxBuffer = await Packer.toBuffer(doc);

    addStep(requestId, "Sending email ‚Ä¶");
    await mailer.sendMail({
      from: `${FROM_NAME} <${FROM_EMAIL}>`,
      to: email,
      replyTo: FROM_EMAIL,
      subject: "ÊÇ®ÁöÑÈÄêÂ≠óÁ®øÔºàÂéüÊñáËàáÁπÅÈ´î‰∏≠ÊñáÔºâ",
      text: `ËΩâÂØ´Â∑≤ÂÆåÊàê ${localStamp}

Êú¨Ê¨°‰∏äÂÇ≥ÊôÇÈï∑ÔºàÁßíÔºâÔºö${jobSeconds}
Ê™îÊ°àÂêçÁ®±Ôºö${fileMeta.originalname}

ÔºàÊúçÂãôÂñÆËôüÔºö${requestId}Ôºâ`,
      attachments: [
        {
          filename: txtName,
          content: attachmentText,
          contentType: "text/plain; charset=utf-8",
        },
        {
          filename: docxName,
          content: docxBuffer,
          contentType:
            "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        },
      ],
    });
    addStep(requestId, "Email sent.");

    // insert transcriptions row
    try {
      const sql = `
        INSERT INTO transcriptions (
          timestamputc, timestamplocal, email,
          jobseconds, cumulativeseconds, minutes, cumulativeminutes,
          filename, filesizemb, language, requestid, processingms,
          succeeded, errormessage, model, filetype
        )
        VALUES (
          $1, $2, $3, $4, $5, $6, $7,
          $8, $9, $10, $11, $12,
          $13, $14, $15, $16
        )
      `;
      const values = [
        new Date(),
        localStamp,
        email,
        jobSeconds,
        cumulativeSeconds,
        minutesForDb,
        cumulativeMinutesForDb,
        fileName,
        fileSizeMB,
        language || "",
        requestId,
        Date.now() - started,
        true,
        "",
        model,
        fileType,
      ];
      await pool.query(sql, values);
      addStep(requestId, "Database record created.");
    } catch (e) {
      addStep(requestId, "‚ö†Ô∏è Database insert failed: " + (e?.message || e));
    }

    await consume({
      event: "transcription.finished",
      status: "succeeded",
      email,
      filename: fileName,
      request_id: requestId,
      job_id: jobId || "",
      token: token || "",
      duration_sec: jobSeconds,
      charged_seconds: jobSeconds,
      language: language || "",
      finished_at: new Date().toISOString(),
    });
    await updateStatus(requestId, "succeeded", jobSeconds); // <-- ADDED
    await setJobStatus(requestId, "done");
    addStep(requestId, "‚úÖ Done");
  } catch (err) {
    const eMsg = err?.message || "Processing error";
    addStep(requestId, "‚ùå " + eMsg);
    await setJobStatus(requestId, "error", eMsg);
    await consume({
      event: "transcription.finished",
      status: "failed",
      email,
      filename: fileName,
      request_id: requestId,
      job_id: jobId || "",
      token: token || "",
      duration_sec: 0,
      charged_seconds: 0,
      language: "",
      finished_at: new Date().toISOString(),
      error: eMsg,
    });
    await updateStatus(requestId, "processing_fail"); // <-- ADDED
  } finally {
    addStep(requestId, "Cleaning up temporary files...");
    for (const f of Array.from(tempFiles)) {
      try {
        if (f && fs.existsSync(f)) fs.unlinkSync(f);
      } catch {}
    }
  }
}

// ---------- routes (ACK-first upload) ----------
app.post(
  "/upload",
  (req, res, next) => {
    upload.single("file")(req, res, function (err) {
      if (err && err.code === "LIMIT_FILE_SIZE") {
        console.error("[/upload] Multer LIMIT_FILE_SIZE:", err);
        return res
          .status(413)
          .json({
            error: `File too large. Max ${Math.round(MAX_UPLOAD_BYTES / (1024 * 1024))} MB.`,
          });
      }
      if (err) {
        console.error("[/upload] Multer error:", err);
        return res.status(400).json({ error: err.message || "Upload error" });
      }
      next();
    });
  },
  async (req, res) => {
    const email = (req.body?.email || "").trim();
    if (!email) return res.status(400).json({ error: "Email is required" });
    if (!req.file) return res.status(400).json({ error: "File is required" });

    const requestId = crypto.randomUUID();

    // Respond first so frontend doesn't see DB blips
    res.status(202).json({ success: true, accepted: true, requestId });

    setImmediate(async () => {
      try {
        try {
          await createJob(requestId);
        } catch (dbErr) {
          console.error(
            `[${requestId}] createJob DB error (continuing):`,
            dbErr?.message || dbErr
          );
        }
        await processJob({
          email,
          inputPath: req.file.path,
          fileMeta: req.file,
          requestId,
          jobId: String(req.body?.job_id || ""),
          token: String(req.body?.token || ""),
        });
      } catch (e) {
        console.error(`[${requestId}] Background crash:`, e?.message || e);
        try {
          await setJobStatus(requestId, "error", e?.message || String(e));
          await updateStatus(requestId, "processing_fail"); // <-- ADDED
        } catch {}
      }
    });
  }
);

app.get("/", (_req, res) =>
  res.send("‚úÖ Whisper backend (upload-only, Postgres) running")
);
const port = process.env.PORT || 3000;
app.listen(port, () => console.log(`üöÄ Server listening on port ${port}`));
